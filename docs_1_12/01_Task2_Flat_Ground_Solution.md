# 项目报告：Task 2 平地盲视行走 (Phase 1)

## 1. 任务概述
**目标**：训练 LimX PointFoot 机器人在平坦地面上进行稳定的盲视行走。
**输入**：仅本体感知 (Proprioception)，即关节位置、速度、IMU 数据。
**输出**：关节位置目标 delta (Action)。
**难点**：无视觉辅助下，机器人极易出现方向漂移、原地打转或步态不稳。

## 2. 踩坑与解决方案 (主要挑战)

### 2.1 致命的“原地打转”问题
*   **现象描述**：
    在初期的配置中，机器人学会了站立，但随后开始疯狂进行原地自旋（Spinning），完全无视直线行走指令。
*   **根本原因分析 (Root Cause)**：
    经过与遗留代码 (`env_cfg_new_only2.py`) 的对比分析，发现旧配置中存在一个极不合理的惩罚项：
    `self.rewards.pen_feet_distance.weight = -50.0` (且目标距离设定极小)。
    这迫使机器人为了减少惩罚，试图将两只脚完全重合在一起，导致物理不稳，机器人发现通过“旋转”可以利用离心力或快速切换支撑腿来最小化这个极其严苛的距离惩罚。
*   **解决方案**：
    1.  **移除惩罚**：彻底移除 `pen_feet_distance` 负奖励。
    2.  **增强约束**：引入“直线强约束”逻辑 (`Straitjacket` 策略)：
        *   `rew_ang_vel_z_precise.weight` 提升至 **5.0** (原为 0.5-1.0)。
        *   这强迫机器人必须保持航向绝对为 0，任何旋转倾向都会导致巨大的奖励损失。

### 2.2 坡面/平地“太空步” (Slipping)
*   **现象描述**：
    机器人在 Play 模式下，腿部摆动看起来在走，但身体几乎不前进，或者在微小的坡度上滑下来。
*   **解决方案**：
    物理摩擦力参数调整。
    ```python
    # 修正前：默认摩擦力
    # 修正后：根据 LimX 物理特性调整
    self.events.robot_physics_material.params["static_friction_range"] = (0.8, 1.2)
    self.events.robot_physics_material.params["dynamic_friction_range"] = (0.6, 0.9)
    ```
    增加摩擦力后，足端抓地力增强，动量传递效率显著提升。

### 2.3 网络架构不匹配 (Shape Mismatch)
*   **现象描述**：
    尝试从 Task 2 (平地) 迁移到 Task 3/4 (复杂地形) 时，或者在 Play 模式加载模型时，报错 `RuntimeError: Shape mismatch`。
*   **原因**：
    *   平地训练不需要“地形高度扫描 (Height Scan)”，输入维度较小 (例如 39)。
    *   复杂地形必须需要高度扫描，输入维度较大 (例如 208)。
    *   PyTorch 无法将 39 维的权重加载到 208 维的网络中。
*   **解决方案 (Forward Compatibility)**：
    在 `PFBlindFlatEnvCfg` (平地配置) 中**强制开启高度扫描雷达**，虽然平地扫描到的全是 0，但这占住了网络输入的坑位 (Placeholders)。
    这确保了 Task 2 的模型可以无缝作为 Task 3 和 Task 4 的预训练模型 (`--resume`)。

## 3. 奖励函数设计思路 (Phase 1 特化)

我们在 Task 2 采用了 **"Foundation First" (基础优先)** 的策略。

| 奖励项 (Reward Term) | 权重 (Weight) | 设计意图 |
| :--- | :--- | :--- |
| **`rew_lin_vel_xy_precise`** | **5.0** | **[核心]** 极高的速度追踪权重。在这个阶段，我们不求步态优美，只求“绝对服从指令”。 |
| **`rew_ang_vel_z_precise`**| **5.0** | **[核心]** 极高的航向锁定权重。严防死守任何旋转漂移，根治“自旋病”。 |
| `keep_balance` | 2.0 | 存活奖励。只要不摔，就给分。鼓励机器人先“活下来”。 |
| `pen_action_smoothness` | -0.05 | **[放宽]** 相比标准值 (-0.1)，我们降低了平滑性惩罚。允许机器人初期动作僵硬一点，优先保证能走直线。 |
| `pen_base_height` | -1.0 | 限制身体高度浮动，防止蹲着走或起飞。 |

## 4. 训练效果分析
*(此处插入您刚才提供的 Task 2 训练曲线截图)*

*   **收敛速度**：
    `Train/mean_episode_length` 在 1000 iter 左右迅速达到最大值，说明机器人很快解决了生存问题。
*   **稳定性**：
    `Train/mean_reward` 曲线在后期极其平稳，没有大幅震荡，证明高权重的速度/航向约束成功地将策略限制在一个稳定的解空间内。
*   **对于后续任务的意义**：
    获得了一个“绝对听话”的行走底座。虽然它没见过楼梯，但它的步态极其规律，为后续微调提供了极佳的初始参数 (Checkpoint)。
