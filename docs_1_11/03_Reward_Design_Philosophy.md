# 模块详解三：奖励管理器 (Reward Manager)

> **目标**：理解如何通过“奖罚分明”来塑造机器人的行为。

## 1. 强化学习的指挥棒

机器人最开始是什么都不会的“随机乱动”。**奖励函数 (Reward Function)** 就是我们告诉它什么是“对”，什么是“错”的唯一方式。

*   **正奖励 (+)**: 做得好！多做这个动作！（例如：活下来、跑到目标速度）
*   **负奖励 (-)**: 做错了！少做这个动作！（例如：摔倒、甚至动作太丑、太费电）

在 `RewardsCfg` 类中定义了所有的规则。

## 2. 核心任务奖励 (Task Rewards)

这是告诉机器人“我们要干什么”。

### 2.1 速度追踪 (Velocity Tracking)
这是目前最重要的任务。
```python
rew_lin_vel_xy_precise = RewTerm(
    func=mdp.track_lin_vel_xy_exp,
    weight=2.0,  # <--- 权重
    params={"std": math.sqrt(0.08)} # <--- 精度要求
)
```
*   **含义**: 如果你的速度接近我命令的目标速度，我就给你加分。
*   **精度 (`std`)**: 这个值越小，要求越严苛。就像射击靶心，`std` 小意味着只有正中红心才给高分，稍微偏一点分就掉得很快。
*   **权重变化**: 代码注释中提到权重从 `8.0` 降到了 `2.0`。这是一个关键调整。如果权重太高，机器人会为了哪怕一点点速度准确度而不惜一切代价（比如抽搐着跑），导致动作变形。降低权重是为了让它“心态更平和”，兼顾其他指标。

### 2.2 姿态稳定性 (Stability)
```python
rew_base_stability = RewTerm(
    func=mdp.flat_orientation_l2,
    weight=1.0,
    ...
)
```
*   **含义**: 奖励机器人保持背部水平。不要像个醉汉一样东倒西歪。

## 3. 辅助惩罚项 (Penalties/Regularization)

这是一些负分项，主要是为了**规范动作风格**，让机器人走得“漂亮”且“省力”。

| 惩罚项名称 | 含义 (小白版) | 为什么要有它？ |
| :--- | :--- | :--- |
| `pen_base_height` | **高度惩罚** | 机器人别蹲太低，也别像长颈鹿一样踮脚尖。保持在 0.78米 的标准高度。 |
| `pen_feet_distance` | **劈叉惩罚** | 两脚别分太开，也别并在一切打架。这能防止机器人走成“外八字”或“内八字”。 |
| `pen_joint_torque` | **力矩惩罚** | 别太用力！我们鼓励省电。如果能轻轻松松走路，就不要在这死命出力。 |
| `pen_action_smoothness` | **平滑惩罚** | 动作别一惊一乍的。这一帧要把腿往前伸，下一帧突然要往后缩，这种神经质的动作要扣分。 |

## 4. 权重调整的艺术 (Reward Engineering)

你会发现代码里有很多注释写着 `[Fix] Reduced from X to Y`。这展示了调试过程中的思考：

*   **梯度爆炸问题**: 如果某一项奖励忽然给了巨大的分值（比如 -50.0 的惩罚），神经网络的更新梯度就会瞬间爆炸，之前的训练可能就白费了。所以把 `pen_feet_distance` 从 -50 降到了 -2.0。
*   **平衡**: 最终的策略是各种奖励博弈的结果。我们在找一个平衡点：既要跑得快（追踪奖励），又要跑得稳（稳定奖励），还要跑得省力（力矩惩罚）。
