# 任务 2.2 详细指南：双足机器人平地速度跟随

**目标受众**：只知道"奖励函数"概念的强化学习新手

**预计阅读时间**：30-40 分钟

---

## 第一部分：基础概念扫盲

### 1.1 什么是强化学习？（5 分钟理解）

你可能听说过强化学习这个概念，但不确定实际在做什么。让我用通俗的方式解释：

```
🎮 类比：训练一个电子游戏AI

传统编程：
  你：写出所有规则 → 电脑：按规则执行
  
强化学习：
  你：告诉电脑什么是"好"（奖励）和"坏"（惩罚）
  电脑：自己尝试各种动作，学习什么动作能得到更多奖励
  结果：电脑学会了一个有效的策略
```

**在这个项目中的应用**：
```
我们的目标：训练一个神经网络，让机器人在听到"向前走 0.5 m/s"时，能精确地走出 0.5 m/s 的速度

流程：
  1️⃣ 机器人做出随机动作（比如随意摆腿）
  2️⃣ 计算奖励：走得快吗？姿态稳定吗？没摔倒吗？
  3️⃣ 神经网络学习：这些动作产生了多少奖励？
  4️⃣ 神经网络优化：找到能产生最大奖励的动作模式
  5️⃣ 重复步骤 1-4，机器人越来越聪明
```

---

### 1.2 PPO 算法简介（10 分钟理解）

PPO = "Proximal Policy Optimization"（近端策略优化）

**你需要知道的关键点**：

| 概念 | 解释 | 在项目中的实现 |
|------|------|--------------|
| **策略 (Policy)** | 根据观测给出动作的神经网络 | Actor 网络（6 维输出） |
| **观测 (Observation)** | 机器人能"看到"的信息 | 59 维向量（位置、速度等） |
| **动作 (Action)** | 机器人执行的指令 | 6 个关节的目标位置 |
| **奖励 (Reward)** | 衡量本次动作好坏的数字 | 多项奖励的加权和 |
| **价值函数 (Value)** | 对"未来总奖励"的预测 | Critic 网络 |

**PPO 如何工作**（简化版）：

```
第 1 步：收集数据 (Rollout)
┌─────────────────────────────────────────┐
│ 用当前策略和 4096 个并行环境            │
│ 执行动作，收集 (观测, 动作, 奖励) 对  │
│ 大约 2048 步 × 4096 环境 = 约 8M 数据 │
└─────────────────────────────────────────┘
              ↓
第 2 步：计算优势 (Advantage)
┌─────────────────────────────────────────┐
│ 优势 = 实际奖励 - 价值函数预测          │
│ 正数 → 这个动作比预期好                │
│ 负数 → 这个动作比预期差                │
└─────────────────────────────────────────┘
              ↓
第 3 步：更新网络 (Update)
┌─────────────────────────────────────────┐
│ 分小批次 (mini-batch) 更新              │
│ - Actor 网络：学习产生高优势的动作     │
│ - Critic 网络：学习更好预测未来奖励    │
│                                        │
│ 重复多轮 (通常 3-5 轮)                 │
└─────────────────────────────────────────┘
              ↓
第 4 步：重复迭代
重复步骤 1-3，直到收敛

整个过程的进度：
第 0 周期：随机行走（奖励很低，约 -0.5）
...
第 100 周期：比较稳定的行走（奖励很高，约 +0.8）
...
第 1000 周期：精确的速度跟随（奖励接近 +1.0）
```

---

### 1.3 神经网络架构（理解一下就好）

```
输入：观测向量 (59 维)
  ↓
┌─────────────────┐
│  隐藏层 1 (512) │  ← MLP_encoder，处理观测信息
│ [ReLU 激活]     │
└─────────────────┘
  ↓
┌─────────────────┐
│  隐藏层 2 (512) │
│ [ReLU 激活]     │
└─────────────────┘
  ↓
  ├─→ Actor Head     ─→  动作均值 (6 维)        → 动作输出
  │                       动作标准差 (6 维)     → 控制探索度
  │
  └─→ Critic Head    ─→  价值预测 (1 维)       → 奖励预测

关键点：
  • Actor 和 Critic 共享前两层（编码器）
    → 加速收敛
  
  • Actor 输出两个量：均值和标准差
    → 均值：机器人的"主要决策"
    → 标准差：机器人的"探索程度"（训练初期高，后期低）
```

---

## 第二部分：项目如何实现任务 2.2？

### 2.1 从观测到动作的完整流程

想象一个时间步（0.005 秒）内发生的事情：

```
时刻 t = 0 ms
│
├─→ 【1】物理仿真获取当前状态
│   ├─ 关节位置：[q1, q2, ..., q6]
│   ├─ 关节速度：[v1, v2, ..., v6]
│   ├─ 基座位置：[x, y, z]
│   ├─ 基座方向：quaternion [qx, qy, qz, qw]
│   ├─ 基座线速度：[vx, vy, vz]
│   ├─ 基座角速度：[wx, wy, wz]
│   └─ 足部接触状态：[contact_L, contact_R]
│
├─→ 【2】观测管理器 (Observation Manager) 构建观测向量
│   ├─ 投影重力向量 (3 dim)
│   │   获取当前姿态相对于重力的偏差
│   │   → 告诉神经网络"机器人现在的倾斜角度"
│   │
│   ├─ 基座角速度 (1 dim)
│   │   获取 Z 轴旋转速度
│   │   → 告诉神经网络"机器人是否在自旋"
│   │
│   ├─ 相对关节位置 (6 dim)
│   │   当前关节位置 - 默认关节位置
│   │   → 告诉神经网络"各个关节距离舒服位置有多远"
│   │
│   ├─ 关节速度 (6 dim)
│   │   各关节的转速
│   │   → 告诉神经网络"各个关节在转动多快"
│   │
│   ├─ 上一步动作 (6 dim)
│   │   前一时刻的神经网络输出
│   │   → 帮助神经网络产生连贯的动作序列
│   │
│   ├─ 步态相位 (2 dim) [sin, cos]
│   │   当前处于步态周期的哪个位置
│   │   → 指导交替的腿部运动
│   │
│   ├─ 步态命令 (3 dim)
│   │   步态的频率、偏移、持续时间
│   │   → 告诉神经网络"应该以什么速度摆腿"
│   │
│   ├─ 高度扫描 (可选，~32 dim)
│   │   前方地形的高度信息
│   │   → 用于障碍物检测
│   │
│   └─ 总计：~59 维观测向量
│
│       示例观测向量：
│       [-0.05, -0.02, 0.98, 0.01, -0.1, 0.2, -0.15, 0.25, ...]
│        ↑      ↑      ↑     ↑     ↑     ↑     ↑      ↑
│      投影重力向量 (3)    | 角速度(1) 相对位置(6) 速度(6)...
│
├─→ 【3】添加观测噪声（模拟传感器误差）
│   原观测：[-0.05, -0.02, 0.98, ...]
│   噪声：  [+0.01, -0.01, -0.02, ...]
│   加噪声后：[-0.04, -0.03, 0.96, ...]
│
│   作用：防止神经网络过拟合真实环境，提高泛化性能
│
├─→ 【4】神经网络前向传播 (Policy Forward Pass)
│   
│   输入：观测向量 (59 维)
│           ↓
│   MLP 编码器 (2 层隐藏层，每层 512 个神经元)
│   隐藏层 1 → ReLU 激活 → 隐藏层 2 → ReLU 激活
│           ↓
│   分裂成两个头部：
│   
│   Actor Head:
│   ├─ 动作均值 (mean): [0.1, -0.05, 0.2, -0.1, 0.15, -0.08]
│   └─ 动作标准差 (std): [0.5, 0.5, 0.5, 0.5, 0.5, 0.5]
│
│   Critic Head:
│   └─ 价值预测: 0.75 (表示"这个观测状态很好，未来应该能获得高奖励")
│
├─→ 【5】采样动作 (Action Sampling)
│   神经网络不是直接输出动作，而是输出动作分布！
│   
│   • 训练早期（探索阶段）：
│     从分布中采样动作，会有较大随机性
│     动作 = 均值 + 标准差 × 随机数
│     例如：动作[0] = 0.1 + 0.5 × 1.2 = 0.7
│     → 机器人会尝试"疯狂的"动作，加快学习
│   
│   • 训练后期（利用阶段）：
│     标准差变小，动作接近均值
│     例如：动作[0] = 0.1 + 0.1 × 0.1 = 0.101
│     → 机器人执行最优策略
│
├─→ 【6】动作管理器 (Action Manager) 处理动作
│   原始动作：[0.1, -0.05, 0.2, -0.1, 0.15, -0.08]
│   │
│   ├─ 缩放：乘以 0.25
│   │  缩放后：[0.025, -0.0125, 0.05, -0.025, 0.0375, -0.02]
│   │
│   ├─ 添加默认位置偏移：加上默认关节位置
│   │  关节目标位置：[0.025, -0.0125, 0.05, -0.025, 0.0375, -0.02]
│   │
│   └─ 转换为 PD 控制指令
│
├─→ 【7】PD 控制器 (关键的物理转换！)
│   
│   目前我们有了"目标关节位置"，但关节执行器需要"扭矩"！
│   
│   PD 控制器的作用：将位置误差转换为力矩
│   
│   公式：τ = K_p × (θ_target - θ_current) + K_d × (ω_target - ω_current)
│   
│   其中：
│   • τ：输出力矩 (N⋅m)
│   • K_p = 25.0：比例增益（刚度）
│   • K_d = 0.8：微分增益（阻尼）
│   • θ：位置角度 (rad)
│   • ω：角速度 (rad/s)
│   
│   例如关节 1：
│   当前位置：0.5 rad
│   目标位置：0.525 rad （神经网络指令）
│   当前速度：0.1 rad/s
│   目标速度：0 rad/s （通常设为 0）
│   
│   位置误差：0.525 - 0.5 = 0.025 rad
│   速度误差：0 - 0.1 = -0.1 rad/s
│   
│   输出力矩：τ = 25.0 × 0.025 + 0.8 × (-0.1)
│             = 0.625 - 0.08
│             = 0.545 N⋅m
│   
│   这个力矩会让关节朝向目标位置运动，同时减少速度
│
│   ⚠️ 重要理解：
│   ├─ K_p 越大：关节反应越快（但容易振荡）
│   ├─ K_d 越大：阻尼越多（但反应变慢）
│   ├─ 找到平衡点很重要！
│
├─→ 【8】力矩限制 (Clipping)
│   计算出的力矩：[0.545, -0.32, 0.81, -0.12, 0.95, -0.48] N⋅m
│   限制范围：[-300, 300] N⋅m （effort_limit）
│   
│   由于计算出的力矩都在范围内，所以不需要裁剪
│   
│   最终应用力矩：[0.545, -0.32, 0.81, -0.12, 0.95, -0.48]
│
└─→ 【9】物理仿真 (Physics Engine)
    应用力矩到关节
    模拟刚体动力学 (Newton's 2nd law)
    检查碰撞
    生成接触力
    
    结果：机器人的关节运动，基座移动
    
    时刻 t = 5 ms（下一个时间步）
    回到【1】，开始新的循环

    ┌────────────────────────────────┐
    │ 整个流程耗时：< 1 毫秒         │
    │ 在 4096 个并行环境中重复       │
    │ 收集大量 (observation, action, │
    │ reward) 数据对用于训练        │
    └────────────────────────────────┘
```

---

### 2.2 奖励函数：机器人学习的指南针

观测和动作很复杂，但奖励函数很关键：**它告诉神经网络"什么是好"**。

#### 当前项目的奖励设计

```python
# 在 limx_base_env_cfg.py 中定义的奖励

总奖励 = 0.5 × stay_alive 
       + 1.0 × base_tracking 
       + 0.5 × gait_reward 
       - 0.1 × feet_regulation 
       - 0.01 × action_smoothness
```

让我详细解释每个奖励项：

#### A. stay_alive（存活奖励）：权重 = 0.5

```
r_stay_alive = +1.0（每一步）

含义：
  只要机器人还没摔倒，就给奖励
  
为什么需要：
  ├─ 防止神经网络学会"原地不动"的策略
  │   （也能避免摔倒，但没有用）
  │
  └─ 鼓励机器人尝试运动
  
权重 0.5 意味着：
  机器人很乐意运动，但不会为了运动而忽视其他目标

调整建议：
  • 如果机器人训练不收敛（经常摔倒）：增加到 1.0
  • 如果机器人太谨慎（走得很慢）：减少到 0.2
```

#### B. base_tracking（速度追踪奖励）：权重 = 1.0 ⭐⭐⭐ 最重要

```
命令速度：v_command = [0.5, 0.0, 0.0]  m/s  (x, y, omega_z)
实际速度：v_actual = [0.48, 0.02, -0.01]  m/s

计算过程：
  速度误差 = |v_actual - v_command|
           = |[0.48 - 0.5, 0.02 - 0.0, -0.01 - 0.0]|
           = |[-0.02, 0.02, -0.01]|
           = sqrt(0.02² + 0.02² + 0.01²)
           = 0.0283 m/s
  
  奖励 = exp(-(速度误差²) / σ²)
       其中 σ = 0.5（容差）
       = exp(-(0.0283²) / 0.5²)
       = exp(-0.0008 / 0.25)
       = exp(-0.0032)
       ≈ 0.9968 ≈ 1.0

含义：
  • 误差接近 0 → 奖励接近 1.0（很好！）
  • 误差 = 0.5 m/s → 奖励 ≈ 0.61（还可以）
  • 误差 = 1.0 m/s → 奖励 ≈ 0.14（很差）
  • 误差 = 2.0 m/s → 奖励 ≈ 0.002（极差）

为什么是主要任务：
  权重 1.0 是所有奖励中最大的
  → 神经网络会优先学习准确跟随速度命令
  
评分标准对应：
  这直接决定了"速度追踪误差"的评分！

调整建议：
  • 如果误差总是 > 0.2 m/s：
    ├─ 增加权重到 2.0 或更高
    ├─ 检查 PD 参数是否合适
    └─ 检查动作空间缩放是否太小
  
  • 如果精度已经很好：
    └─ 可以保持 1.0，调整其他奖励
```

#### C. gait_reward（步态奖励）：权重 = 0.5

这是最复杂的奖励项，涉及机器人的步态质量。

```
步态的关键是"接触模式"：
  
期望的接触模式（二足步态）：
  时刻：   0ms     50ms    100ms   150ms   200ms
        ├────────┤
  左腿：   接触     摆动     摆动    接触    接触
  右腿：   摆动     摆动    接触    接触    摆动

步态奖励计算（简化版）：
  
  对于每条腿：
    • 应该接触时（支撑相）
      ├─ 检查是否有接触力
      ├─ 如果有力 → 给奖励 +reward
      └─ 如果没力 → 不给奖励
    
    • 应该摆动时（摆动相）
      ├─ 检查足部高度是否足够
      ├─ 如果足够高 → 给奖励 +reward
      └─ 如果太低 → 给惩罚 -penalty

为什么需要：
  ├─ 只有 base_tracking 时，机器人可能会
  │   "滑行"（腿不动，直接滑动）或"跳跃"
  │
  └─ gait_reward 强制机器人使用合理的步态
    （交替抬腿，稳定行走）

影响：
  ├─ 权重太小：机器人步态奇怪，但速度可能更快
  ├─ 权重太大：机器人步态很漂亮，但速度控制精度差
  └─ 权重 0.5：平衡点（推荐）

调整建议：
  • 如果看起来"不像走路"（比如单腿跳跃）：增加到 1.0
  • 如果步态很好但速度精度差：减少到 0.3
```

#### D. feet_regulation（足部调节惩罚）：权重 = -0.1

```
惩罚的对象：足部位置不稳定

计算过程（简化）：
  足部应该在哪里？
    → 在地面上！（z 坐标约 = 基座高度 - 腿长）
  
  当前足部高度与期望的偏差
  → 计算偏差（越小越好）
  
  奖励 = -偏差  （负数表示惩罚）

为什么是负数（惩罚）：
  ├─ 我们不想鼓励这个行为
  ├─ 只是想阻止过度的足部抖动
  └─ 权重很小（0.1），影响有限

例子：
  足部高度偏差 = 0.5 cm
  惩罚 = -0.1 × 0.005 = -0.0005
  
  这个惩罚几乎可以忽略不计！
  
调整建议：
  • 如果足部抖动明显：增加权重到 -0.5
  • 如果已经很稳定：保持 -0.1
```

#### E. action_smoothness（动作平滑性惩罚）：权重 = -0.01

```
惩罚的对象：动作的剧烈变化

为什么要平滑：
  ├─ 现实中电机有惯性，不能瞬间改变输出
  ├─ 平滑的动作更接近现实
  ├─ 可以减少能耗
  └─ 有助于 sim-to-real 转换

计算方式：
  动作加速度 = (a[t] - 2*a[t-1] + a[t-2]) / dt²
  
  如果神经网络输出在瞬间大幅变化
  → 这个惩罚就会大
  
  例如：
    t-2: a = [0.1, 0.1, ...]
    t-1: a = [0.1, 0.1, ...]
    t:   a = [0.9, 0.9, ...]  ← 突然改变！
    
    加速度很大 → 惩罚很大

权重的意义：
  -0.01 意味着这个惩罚几乎微不足道
  → 神经网络不会为了平滑而牺牲速度追踪精度

调整建议：
  • 如果动作很抖：增加到 -0.1 或 -0.5
  • 如果已经很平滑：保持 -0.01
```

---

### 2.3 完整的训练循环（核心过程）

让我展示从"开始训练"到"任务完成"的整个过程：

```
┌─────────────────────────────────────────────────────────────┐
│ 训练循环：Episode = 一个完整的"尝试"                      │
│         通常持续 2500 时间步 ≈ 12.5 秒                    │
└─────────────────────────────────────────────────────────────┘

✋ 初始化 (Initialization)
  ├─ 加载机器人模型（USD 文件）
  ├─ 创建 4096 个并行环境
  ├─ 初始化策略网络（随机权重）
  ├─ 初始化价值网络（随机权重）
  └─ 设置地面、地形、光照等

第 0 轮  ← Episode 0
  状态：机器人行为完全随机，奖励很低（约 -10）
  
  步骤 1-2500：数据收集
    • 每步生成 4096 个 (observation, action, reward)
    • 总共收集 ~ 8M 数据点
    • 计算优势函数
  
  数据更新 (Policy Update)
    • 分 mini-batch 处理（通常 batch_size=256）
    • 每个 mini-batch 训练多遍（通常 3-5 遍）
    • 使用 PPO 损失函数更新网络
    • 学习率：通常 5e-4
  
  结果：策略网络稍微改进，能获得更多奖励（约 -8）

第 1-10 轮
  机器人开始理解"应该运动"
  奖励逐渐提高：-8 → -5 → -2 → 0
  策略变化：从完全随机 → 有轻微的运动方向

第 11-50 轮
  关键阶段：机器人学习如何控制速度
  奖励：0 → 0.3 → 0.5
  观察：机器人开始有规律地摆腿，能向前走

第 51-100 轮
  微调阶段：优化步态和速度精度
  奖励：0.5 → 0.7 → 0.85
  观察：机器人走路更稳定，能跟随多个速度命令

第 101-500 轮
  收敛阶段：奖励趋于稳定
  奖励：0.85 → 0.92 → 0.95
  观察：
    ✓ 能精确追踪直线速度指令
    ✓ 能响应转向命令
    ✓ 姿态相对稳定
    ✓ 步态流畅

第 501-1000 轮
  成熟阶段：性能稳定
  奖励：维持 0.95+
  观察：
    ✓ 能处理不同速度和转向的任意组合
    ✓ 响应灵敏
    ✓ 极少摔倒

✅ 训练完成！

性能指标总结：
  • 总体奖励：> 0.9
  • 速度追踪误差：< 0.1 m/s（通常）
  • 姿态稳定性：Roll/Pitch 振幅 < 10°
  • 成功率：99% 以上不摔倒
```

---

## 第三部分：如何完成任务 2.2

### 3.1 评分标准回顾

```
评分标准：
  1. 速度追踪误差 (MSE)
     ├─ 误差 < 0.05 m/s：100 分
     ├─ 误差 0.05-0.1 m/s：90 分
     ├─ 误差 0.1-0.2 m/s：70 分
     └─ 误差 > 0.2 m/s：50 分
  
  2. 姿态稳定性
     ├─ Roll/Pitch < 5°：100 分
     ├─ Roll/Pitch 5-10°：80 分
     ├─ Roll/Pitch 10-20°：50 分
     └─ Roll/Pitch > 20°：0 分
  
  3. 存活率
     ├─ 不摔倒：100 分
     ├─ 摔倒 < 5%：80 分
     ├─ 摔倒 5-20%：50 分
     └─ 摔倒 > 20%：0 分
```

### 3.2 实际的优化流程

#### 阶段 1：验证基础设置（第 1 天）

**目标**：确保代码能运行，训练能进行

```bash
# 1. 启动训练
cd rsl_rl
python scripts/train.py --task=PointFootLocomotion --headless

# 2. 监控输出（应该看到）：
#    Iteration 1/1000
#    Episode 0-10:  Reward = -10.0
#    Iteration 2/1000
#    Episode 10-20: Reward = -8.5
#    ...
#    Iteration 10/1000
#    Episode 90-100: Reward = 0.3

# 3. 预期时间：
#    • 第 1 轮（0-10 episode）：5-10 分钟
#    • 第 10 轮（90-100 episode）：50-100 分钟
#    ⚠️ 如果运行时间明显更长，检查 GPU 是否被使用
```

**可能的问题和解决方案**：

```
问题 1：运行非常慢（每轮 > 30 分钟）
原因：GPU 未被使用
解决：
  1. 检查 CUDA 是否安装
  2. 检查 PyTorch 是否使用 CUDA
  3. 尝试更新驱动

问题 2：内存不足错误
原因：4096 个环境太多
解决：
  在 limx_base_env_cfg.py 中修改：
  num_envs = 2048  # 从 4096 减少到 2048
  或者 1024

问题 3：出现 NaN 或奖励突然下降
原因：超参数设置问题或数值不稳定
解决：
  检查学习率、梯度裁剪等参数
```

---

#### 阶段 2：分析初始性能（第 1-2 天）

训练运行后，观察前 100 轮的学习曲线：

```
理想的学习曲线：
                      奖励
                        ↑
                    1.0 |      ___________
                        |    /
                    0.5 |  /
                        |/___
                      0 |_______
                        |
                   -0.5 |
                        |__________|____________→ 周期
                        0  50   100  150  200

这表示：
  • 前 50 周期：快速学习（随机 → 有些理解）
  • 50-100 周期：稳定学习（不断优化）
  • 100+ 周期：收敛（性能稳定）

异常的学习曲线 1（奖励不稳定）：
                        ↑
                    1.0 |  /\/\/\/
                        |/
                      0 |
                        |
                   -0.5 |
                        |__________|____________→ 周期

原因：学习率太大，或奖励函数不稳定
解决：降低学习率、调整奖励权重

异常的学习曲线 2（奖励无进展）：
                        ↑
                      0 |________________
                        |
                   -0.5 |
                        |
                   -1.0 |
                        |__________|____________→ 周期

原因：
  • 学习率太小
  • 奖励设置问题
  • 任务本身过难
解决：检查奖励函数、增加相关权重
```

---

#### 阶段 3：优化速度追踪性能（第 2-3 天）

一旦基础设置工作，就开始优化评分标准。

**关键文件**：`exts/bipedal_locomotion/bipedal_locomotion/tasks/locomotion/mdp/rewards.py`

```python
# 当前的 base_tracking 奖励函数（简化版）

def base_tracking(env, asset_cfg):
    """速度追踪奖励"""
    asset = env.scene[asset_cfg.name]
    
    # 获取实际速度（只看 x, y 平面）
    base_lin_vel = asset.data.root_lin_vel_w[:, :2]
    
    # 获取命令速度
    commands = env.command_manager.get_command("base_velocity")
    lin_commands = commands[:, :2]
    
    # 计算速度误差
    lin_vel_error = torch.norm(base_lin_vel - lin_commands, dim=1)
    
    # 高斯奖励（误差越小越好）
    reward = torch.exp(-lin_vel_error ** 2 / 0.5)
    
    return reward

# ✅ 优化想法 1：改变高斯函数的宽度
# 原来：exp(-e² / 0.5)  → 对误差敏感
# 新的：exp(-e² / 0.25) → 对误差更敏感（更严格）
#       exp(-e² / 1.0)  → 对误差不敏感（更宽松）

# 建议：从 0.5 开始，根据实际误差调整
#       如果误差总是 > 0.1 m/s，改为 1.0
#       如果误差总是 < 0.02 m/s，改为 0.25

# ✅ 优化想法 2：添加旋转速度追踪
# 原来：只跟踪线速度 (v_x, v_y)
# 新的：也跟踪旋转速度 (omega_z)

def base_tracking_improved(env, asset_cfg):
    """改进的速度追踪奖励"""
    asset = env.scene[asset_cfg.name]
    
    # 线速度奖励
    base_lin_vel = asset.data.root_lin_vel_w[:, :2]
    commands = env.command_manager.get_command("base_velocity")
    lin_commands = commands[:, :2]
    lin_vel_error = torch.norm(base_lin_vel - lin_commands, dim=1)
    lin_reward = torch.exp(-lin_vel_error ** 2 / 0.5)
    
    # 角速度奖励（新增）
    base_ang_vel = asset.data.root_ang_vel_w[:, 2]  # Z 轴
    ang_commands = commands[:, 2]
    ang_vel_error = torch.abs(base_ang_vel - ang_commands)
    ang_reward = torch.exp(-ang_vel_error ** 2 / 0.2)  # 更严格
    
    # 组合
    return 0.8 * lin_reward + 0.2 * ang_reward
```

**优化步骤**：

```
1️⃣ 查看当前性能
   运行训练到 200 轮
   记录：• 平均奖励
         • 速度误差
         • 是否能跟随转向命令

2️⃣ 根据问题调整
   
   问题：直线走不稳，误差 > 0.15 m/s
   解决：
     • 增加 base_tracking 权重从 1.0 → 1.5
     • 或调整高斯宽度从 0.5 → 0.3
     • 重新训练 50 轮看效果
   
   问题：转向不响应（总是直线走）
   解决：
     • 添加角速度追踪（如上面的代码）
     • 在 limx_base_env_cfg.py 中注册新函数
     • 设置权重 0.5
   
   问题：走路中途摔倒
   解决：
     • 增加 stay_alive 权重从 0.5 → 1.0
     • 增加 feet_regulation 权重从 -0.1 → -0.3
     • 增加 gait_reward 权重从 0.5 → 1.0

3️⃣ 迭代优化
   调整一个参数 → 训练 50 轮 → 查看性能
   重复，直到达到目标
```

---

#### 阶段 4：提升姿态稳定性（第 3 天）

```
症状：机器人走路时晃晃悠悠，Roll/Pitch > 15°

原因分析：
  • PD 控制器反应不足（刚度太小）
  • 步态不稳定
  • 平衡反馈不足

解决方案：

方案 A：调整 PD 参数（在 pointfoot_cfg.py 中）
  原来：stiffness=25.0, damping=0.8
  改为：stiffness=35.0, damping=1.2
  
  效果：关节更硬，反应更快，姿态更稳
  
  ⚠️ 副作用：可能过于僵硬，能耗增加

方案 B：增加平衡奖励（在 rewards.py 中添加）
  
  新的奖励项：
  ```python
  def roll_pitch_penalty(env, asset_cfg):
      """惩罚过大的倾斜"""
      asset = env.scene[asset_cfg.name]
      
      # 获取欧拉角
      euler = quaternion_to_euler(asset.data.root_quat_w)
      roll = euler[:, 0]
      pitch = euler[:, 1]
      
      # 惩罚过大的倾斜
      penalty = torch.abs(roll) + torch.abs(pitch)
      
      return -penalty
  ```
  
  在 limx_base_env_cfg.py 中注册：
  ```python
  roll_pitch_stability = RewTerm(
      func=mdp.roll_pitch_penalty,
      weight=-0.5,  # 增加惩罚力度
  )
  ```

方案 C：优化步态（增加 gait_reward 权重）
  原来：gait_reward 权重 = 0.5
  改为：gait_reward 权重 = 1.0
  
  效果：更规律的步态 → 更稳定的姿态
```

---

#### 阶段 5：减少摔倒（第 3-4 天）

```
症状：训练到 50 轮时，机器人经常摔倒

原因分析：
  • 学习不足（收敛不完全）
  • 奖励函数不支持稳定行走
  • 地形干扰

快速修复：

1️⃣ 立即增加稳定性奖励
   在 limx_base_env_cfg.py 中：
   
   stay_alive = RewTerm(
       func=mdp.stay_alive,
       weight=2.0,  # 从 0.5 增加到 2.0
   )
   
   feet_regulation = RewTerm(
       func=mdp.feet_regulation,
       weight=-0.5,  # 从 -0.1 增加到 -0.5
   )

2️⃣ 添加检测摔倒的条件
   在环境中检查 Base 的 z 坐标
   如果 z < 0.1（或某个阈值），判定摔倒
   摔倒时：
     • 给予大的负奖励
     • 重置环境（新的一轮）

3️⃣ 添加摔倒检测的事件回调
   在 events.py 中添加：
   
   ```python
   def reset_on_fall(env):
       """检测摔倒并重置"""
       asset = env.scene["robot"]
       base_z = asset.data.root_pos_w[:, 2]
       
       # 如果基座低于 0.3 m，认定摔倒
       fell_indices = torch.where(base_z < 0.3)[0]
       
       if len(fell_indices) > 0:
           env.reset(fell_indices)
   ```
```

---

### 3.3 逐步优化的完整检查清单

```
第 1 天：基础验证
  ☐ 代码能运行
  ☐ 出现学习曲线（奖励随时间增加）
  ☐ 第 10 轮时，奖励应该 > -1.0

第 2 天：速度追踪优化
  ☐ 第 100 轮时，机器人能稳定向前走
  ☐ 误差 < 0.2 m/s（初步要求）
  ☐ 能响应简单的速度命令
  ☐ 检查学习曲线，应该接近收敛

第 3 天：性能微调
  ☐ 误差 < 0.1 m/s（目标要求）
  ☐ 姿态稳定，Roll/Pitch < 10°
  ☐ 存活率 > 95%
  ☐ 响应转向命令（如果实现了）

第 4 天：最终优化
  ☐ 误差 < 0.05 m/s（如果可能）
  ☐ 姿态稳定，Roll/Pitch < 5°
  ☐ 存活率 > 99%
  ☐ 测试多种命令组合

部署前检查：
  ☐ 训练曲线稳定（无异常波动）
  ☐ 保存的模型文件完整
  ☐ 测试脚本能正确加载模型
  ☐ 在多个测试中性能一致
```

---

## 第四部分：代码实战指南

### 4.1 修改奖励权重的步骤

假设你想增加速度追踪的权重，因为误差太大。

**文件**：`exts/bipedal_locomotion/bipedal_locomotion/tasks/locomotion/cfg/PF/limx_base_env_cfg.py`

```python
# 找到以下部分（通常在文件的最后部分）

@configclass
class RewardsCfg:
    """奖励配置"""
    
    # 原来
    base_tracking = RewTerm(
        func=mdp.base_tracking,
        weight=1.0,  # ← 这个值
        ...
    )
    
    # 改为
    base_tracking = RewTerm(
        func=mdp.base_tracking,
        weight=2.0,  # 增加权重（通常增加 1 倍）
        ...
    )
```

**验证修改**：

```bash
# 1. 保存文件
# 2. 启动新的训练
python scripts/train.py --task=PointFootLocomotion --headless

# 3. 对比新旧训练
#    原来 100 轮：奖励 = 0.65，误差 = 0.18 m/s
#    新的 100 轮：奖励 = 0.72，误差 = 0.12 m/s
#              ↓ 改善了！
```

---

### 4.2 添加新的奖励项的步骤

假设你想添加一个"角速度追踪"奖励。

**第 1 步**：在 `rewards.py` 中实现函数

```python
# 文件：exts/bipedal_locomotion/bipedal_locomotion/tasks/locomotion/mdp/rewards.py

def angular_velocity_tracking(env: ManagerBasedRLEnv,
                             asset_cfg: SceneEntityCfg = SceneEntityCfg("robot"),
                            ) -> torch.Tensor:
    """追踪角速度命令"""
    asset: Articulation = env.scene[asset_cfg.name]
    
    # 获取实际角速度（Z 轴）
    base_ang_vel = asset.data.root_ang_vel_w[:, 2]
    
    # 获取命令角速度
    commands = env.command_manager.get_command("base_velocity")
    ang_commands = commands[:, 2]
    
    # 计算误差
    ang_vel_error = torch.abs(base_ang_vel - ang_commands)
    
    # 高斯奖励
    reward = torch.exp(-ang_vel_error ** 2 / 0.2)
    
    return reward
```

**第 2 步**：在 `limx_base_env_cfg.py` 中注册

```python
# 文件：exts/bipedal_locomotion/bipedal_locomotion/tasks/locomotion/cfg/PF/limx_base_env_cfg.py

@configclass
class RewardsCfg:
    """奖励配置"""
    
    # ... 其他奖励 ...
    
    # 新增
    angular_velocity_tracking = RewTerm(
        func=mdp.angular_velocity_tracking,
        weight=0.3,  # 小权重，因为是新增项
    )
```

**第 3 步**：重新训练

```bash
python scripts/train.py --task=PointFootLocomotion --headless

# 现在机器人应该能更好地跟踪转向命令
```

---

### 4.3 调试的黄金法则

| 问题 | 检查顺序 | 修复方案 |
|------|--------|--------|
| 奖励不增长 | 1. 观测是否正确 2. 学习率 3. 奖励权重 | 增加主任务权重，检查学习率 |
| 摔倒频繁 | 1. 地形 2. PD 参数 3. 奖励 | 增加存活奖励，检查 PD 刚度 |
| 误差大 | 1. 奖励权重 2. 动作缩放 3. PD 参数 | 增加主任务权重 |
| 动作抖动 | 1. 动作平滑惩罚 2. 学习率 3. 网络大小 | 增加平滑惩罚权重 |
| 没有进展 | 1. 学习率 2. 梯度裁剪 3. 网络初始化 | 试着减小学习率或增加训练时间 |

---

## 第五部分：理解关键的数学公式

### 5.1 PPO 损失函数（了解即可）

```
L^PPO = E[L^clip + c1 * L^VF + c2 * S]

其中：
• L^clip = 策略损失（PPO 的核心创新）
• L^VF = 价值函数损失
• S = 熵奖励（鼓励探索）
• c1, c2 = 权重系数

不需要完全理解，只需知道：
PPO 会让"好的动作"更可能被选中
PPO 会让神经网络学会预测未来奖励
PPO 会保持一定的随机性（用于探索）
```

### 5.2 高斯奖励函数

```
奖励 = exp(-(误差²) / σ²)

例子：
误差 = 0 m/s    → 奖励 = exp(0) = 1.0 ✓✓✓
误差 = 0.1 m/s  → 奖励 = exp(-0.04) ≈ 0.96 ✓✓
误差 = 0.2 m/s  → 奖励 = exp(-0.16) ≈ 0.85 ✓
误差 = 0.5 m/s  → 奖励 = exp(-1) ≈ 0.37 △
误差 = 1.0 m/s  → 奖励 = exp(-4) ≈ 0.018 ✗

σ 的作用：
σ = 0.5（较宽松）：即使误差 0.5 m/s 也有 37% 奖励
σ = 0.1（严格）：误差 0.2 m/s 只有 36% 奖励

所以：
• σ 大 → 容差大，更容易得奖励（有利于快速学习）
• σ 小 → 容差小，追求精度（有利于精细控制）
```

---

## 第六部分：常见错误和陷阱

### ❌ 错误 1：一下子改太多参数

```
错误做法：
  • 把所有权重都改了
  • 改 PD 参数、学习率、网络结构同时进行
  → 无法判断哪个改变造成了后果

正确做法：
  • 每次只改一个参数
  • 训练几十轮看效果
  • 如果好转，保留；如果变差，还原
```

### ❌ 错误 2：忘记保存模型

```
错误做法：
  • 训练一周
  • 忘记保存模型
  • 计算机重启
  → 一切付之东流

正确做法：
  # 在 train.py 中
  checkpoint = runner.save(...)  # 自动保存
  
  # 或手动指定
  torch.save(policy.state_dict(), "my_model.pt")
```

### ❌ 错误 3：混淆"惩罚"和"奖励"

```
错误想法：
  • 我想减少某个行为，所以权重设为 -1.0
  
正确理解：
  • 权重 -1.0 表示这个项被重视，但方向相反
  • 对于"坏行为"应该：
    • 给负的奖励值（函数返回负数）
    • 配合正的权重
    • 或者给正的奖励值，配合负的权重

例子：
# 错误
action_smoothness = RewTerm(
    func=mdp.action_smoothness,  # 返回负数
    weight=-1.0,  # 权重也是负数
)  # 结果：-负数 = 正数，反而鼓励抖动！

# 正确 1
action_smoothness = RewTerm(
    func=mdp.action_smoothness,  # 返回负数
    weight=1.0,  # 权重为正
)  # 结果：负数 × 1.0 = 负数，惩罚抖动 ✓

# 正确 2
action_smoothness = RewTerm(
    func=mdp.action_smoothness_positive,  # 返回正数
    weight=-1.0,  # 权重为负
)  # 结果：正数 × -1.0 = 负数，惩罚抖动 ✓
```

### ❌ 错误 4：奖励值过大

```
错误做法：
  stay_alive = RewTerm(func=..., weight=100.0)
  base_tracking = RewTerm(func=..., weight=1000.0)
  
后果：
  • 数值不稳定
  • 梯度爆炸或消失
  • 训练失败

正确做法：
  • 所有权重应该在 [-2, 2] 范围内
  • 通常在 [-0.5, 1.5] 范围内
```

---

## 总结：你需要做什么

### 快速开始（1 小时）

```bash
# 1. 运行基础训练
python scripts/train.py --task=PointFootLocomotion --headless

# 2. 让它运行 2-3 小时
# 3. 观察学习曲线

# 4. 检查结果
python scripts/play.py --task=PointFootLocomotion \
    --checkpoint=logs/PointFootLocomotion-PointFootLocomotionPPO/*/model.pt
```

### 进阶优化（1-3 天）

```
Day 1:
  • 修改奖励权重
  • 重新训练观察效果
  
Day 2:
  • 调整 PD 参数
  • 添加新奖励项（如角速度追踪）
  • 微调高斯函数宽度
  
Day 3:
  • 优化收敛速度
  • 增强姿态稳定性
  • 测试多个命令组合
```

### 部署前（30 分钟）

```
• 测试最终模型性能
• 验证评分标准都达到
• 保存最终模型
• 备份所有配置和代码
```

---

**下一步**：继续学习任务 2.3（抗干扰）和 2.4（复杂地形）时，这些基础知识都会用上！

